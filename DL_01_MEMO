""" ANN(Artificial Neural Network)
- 인간 개입의 여부
  - ML: 인간이 직접 특징을 도출할 수 있게 설계
  - DL: 스스로 일정 범주의 데이터를 바탕으로 공통된 특징을 도출
- 퍼셉트론(Perceptron)
  - XOR 문제 -> 복층 퍼셉트론으로 해결
- 순전파(forward propagation) / 역전파(back propagation)
  - 최적화된 가중치를 찾아가며 학습하는 방식 -> 오차가 최소가되는 방향
- 활성함수(Activation Func.)
  - 가중치 생성
  - 선형 결합을 비선형(또는 선형) 결합으로 변환하는 역할
- Gradient Vanishing (기울기 소실): 역전파 과정에서 입력층으로 갈수록 기울기가 점차 작아지는 현상
- Gradient Exploding (기울기 폭주): 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되며 발산하는 현상
- 딥러닝의 학습과정
  - 출력값과 실제값을 비교해, 그 차이를 최소화하는 가중치(W)와 편향(bias)의 조합 찾기
  - 가중치는 오차를 최소화하는 방향으로 모델이 스스로 탐색(역전파)
  - 오차 계산은 실제 데이터를 비교해, 손실함수를 최소화하는 값 탐색
  - 옵티마이저로 경사하강법 원리를 이용
- 손실함수(Loss Func.)
  - 실제값과 예측값의 차이를 수치화해주는 함수
  - Crossentropy는 값이 낮을수록 예측을 잘한것임
  - 분류
    - 이항분류: BinaryCrossentropy (이진 엔트로피)
    - 다항분류
      - CategoricalCrossentropy (교차 엔트로피)
      - CategoricalHinge (범주형 힌지 손실)
      - Hinge (힌지 손실)
      - KLDivergence (Kullback-Leibler 발산 손실)
  - 회귀
    - CosineSimilarity (코사인 유사도)
    - Huber (Huber 손실)
    - LogCosh (예측 오차의 쌍곡선 코사인의 로그)
    - MeanAbsoluteErr
    - MeanAbsulutePercentageErr
- 옵티마이저 (Optimizer)
  - 데이터와 손실함수를 기반으로 모델이 업데이트되는 방식
- 경사하강법 (Gradient Descent)
  - 가중치를 움직이며 최솟값을 찾는 방법 (최적화 방법)
- 확률적 경사하강법(SGD, Stochastic Gradient Descent)
  - 랜덤하게 추출한 일부 데이터에 대해 가중치를 조절
  - 속도는 개선되었지만, 극소 문제(loval minima)가 남아있음
- ANN(Artificial Neural Network)
  - 입력층(Input) -> 은닉층(Hidden) -> 출력층(Output)
- DNN(Deep Neural Network)
  - ANN을 기반으로 은닉층을 많이 늘림
- 출력층에서의 활성화 함수
  - 분류: Sigmoid, Softmax
  - 회귀: 항등함수(identity func.)
- 밀집층(Dense layer, FC)
  - 모든 뉴런에 연결되는 계층
  - 과적합 확률이 높아, 드롭아웃(Dropout) 기법을 이용
- loss: categorical_crossentropy = 원핫 벡터 라벨
- loss: sparse_categorical_crossentropy = 정수형 라벨 

NN: 인공 신경망 기계학습 알고리즘.(단점: 최적 파라미터 찾기 어렵다. overfitting, 학습시간이 상대적으로 느림)
DNN: 모델내 은닉층을 많이 늘려서 학습 결과 향상시키는 알고리즘.
CNN: 데이터의 특징을 추출하여 특징의 패턴을 파악하는 알고리즘.(CL, PL 복합적으로 구성)
RNN: 내부에 순환구조가 있는 알고리즘. 현재의 학습과 과거의 학습이 영향을 주고받음.
활성화함수: 입력된 데이터의 가중 합을 출력신호로 변환하는 함수.

기울기 소실 문제: 역전파 알고리즘에서 처음 입력층으로 진행할수록 기울기가 점차적으로 작아지다
점점 변화가 없어지는 문제를 말함.(해결방법 ReLU, Leaky ReLU 사용) """
